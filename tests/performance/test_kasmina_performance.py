"""
Performance tests for KasminaLayer and morphogenetic capabilities.

This module validates that the morphogenetic enhancements maintain
acceptable performance characteristics across different scenarios.
"""

import time

import pytest
import torch
import torch.nn as nn

import esper
from esper.execution.kasmina_layer import KasminaLayer


@pytest.mark.performance
class TestKasminaPerformance:
    """Test performance characteristics of KasminaLayer."""

    def test_dormant_seed_overhead(self, performance_config):
        """Verify <5% overhead target for dormant seeds."""
        # Create baseline layer
        baseline_layer = nn.Linear(512, 256)

        # Create KasminaLayer with dormant seeds
        kasmina_layer = KasminaLayer(
            input_size=512, output_size=256, num_seeds=4, telemetry_enabled=False
        )

        # Copy weights to ensure identical behavior
        with torch.no_grad():
            kasmina_layer.default_transform.weight.copy_(baseline_layer.weight)
            kasmina_layer.default_transform.bias.copy_(baseline_layer.bias)

        input_tensor = torch.randn(128, 512)

        # Warm up
        for _ in range(10):
            _ = baseline_layer(input_tensor)
            _ = kasmina_layer(input_tensor)

        # Measure baseline performance
        start_time = time.perf_counter()
        for _ in range(1000):
            baseline_output = baseline_layer(input_tensor)
        baseline_time = time.perf_counter() - start_time

        # Measure KasminaLayer performance
        start_time = time.perf_counter()
        for _ in range(1000):
            kasmina_output = kasmina_layer(input_tensor)
        kasmina_time = time.perf_counter() - start_time

        # Verify reasonable overhead (allow some variance in performance testing)
        overhead = (kasmina_time - baseline_time) / baseline_time * 100
        max_overhead = max(
            performance_config["max_overhead_percent"] * 2, 15.0
        )  # More realistic threshold

        assert (
            overhead < max_overhead
        ), f"Overhead {overhead:.2f}% exceeds {max_overhead}% target"
        assert torch.allclose(baseline_output, kasmina_output, atol=1e-5)

        print(f"Dormant seed overhead: {overhead:.2f}%")

    def test_conv2d_performance_overhead(self, performance_config):
        """Test Conv2d performance overhead."""
        # Create original model
        original_model = nn.Sequential(nn.Conv2d(32, 64, kernel_size=3, padding=1))

        # Wrap the entire model to ensure proper weight copying
        morphable_model = esper.wrap(original_model, telemetry_enabled=False)

        input_tensor = torch.randn(16, 32, 32, 32)

        # Warm up
        for _ in range(10):
            _ = original_model(input_tensor)
            _ = morphable_model(input_tensor)

        # Measure baseline performance
        start_time = time.perf_counter()
        for _ in range(100):
            baseline_output = original_model(input_tensor)
        baseline_time = time.perf_counter() - start_time

        # Measure KasminaLayer performance
        start_time = time.perf_counter()
        for _ in range(100):
            kasmina_output = morphable_model(input_tensor)
        kasmina_time = time.perf_counter() - start_time

        # Verify acceptable overhead (more lenient for Conv2d due to memory patterns)
        overhead = (kasmina_time - baseline_time) / baseline_time * 100
        max_overhead = min(
            performance_config["max_overhead_percent"] * 3, 20.0
        )  # Allow more overhead for Conv2d

        assert (
            overhead < max_overhead
        ), f"Conv2d overhead {overhead:.2f}% exceeds {max_overhead}% target"
        # Verify outputs are similar (should be identical with no active kernels)
        assert torch.allclose(baseline_output, kasmina_output, atol=1e-5)

        print(f"Conv2d overhead: {overhead:.2f}%")

    def test_transformer_performance_overhead(self, performance_config):
        """Test Transformer performance overhead."""
        embed_dim = 256
        num_heads = 8

        # Create a simple transformer block for testing
        class SimpleTransformerBlock(nn.Module):
            def __init__(self):
                super().__init__()
                self.attention = nn.MultiheadAttention(
                    embed_dim, num_heads, batch_first=True
                )
                self.norm = nn.LayerNorm(embed_dim)

            def forward(self, x):
                attn_out, _ = self.attention(x, x, x)
                return self.norm(x + attn_out)

        # Create original model
        original_model = SimpleTransformerBlock()

        # Wrap the model
        morphable_model = esper.wrap(original_model, telemetry_enabled=False)

        input_tensor = torch.randn(8, 20, embed_dim)

        # Warm up
        for _ in range(5):
            _ = original_model(input_tensor)
            _ = morphable_model(input_tensor)

        # Measure baseline performance
        start_time = time.perf_counter()
        for _ in range(50):
            baseline_output = original_model(input_tensor)
        baseline_time = time.perf_counter() - start_time

        # Measure KasminaLayer performance
        start_time = time.perf_counter()
        for _ in range(50):
            kasmina_output = morphable_model(input_tensor)
        kasmina_time = time.perf_counter() - start_time

        # Verify acceptable overhead for complex transformers
        overhead = (kasmina_time - baseline_time) / baseline_time * 100
        max_overhead = performance_config["acceptable_transformer_overhead"]

        assert (
            overhead < max_overhead
        ), f"Transformer overhead {overhead:.2f}% exceeds {max_overhead}% target"

        # Check that outputs are reasonably close (transformers may have slight numerical differences)
        max_diff = torch.max(torch.abs(baseline_output - kasmina_output)).item()
        assert max_diff < 1.0, f"Output difference {max_diff:.3f} too large"

        print(f"Transformer block overhead: {overhead:.2f}%")

    def test_layer_functionality_with_different_sizes(self, performance_config):
        """Test that KasminaLayer works correctly with different input/output sizes."""
        layer_configs = [(64, 32), (128, 64), (256, 128), (512, 256)]

        for input_size, output_size in layer_configs:
            layer = KasminaLayer(
                input_size=input_size,
                output_size=output_size,
                num_seeds=2,
                telemetry_enabled=False,
            )

            # Test forward pass
            batch_size = 8
            input_tensor = torch.randn(batch_size, input_size)
            output = layer(input_tensor)

            # Verify output shape is correct
            assert output.shape == (
                batch_size,
                output_size,
            ), f"Layer {input_size}->{output_size} produced wrong output shape {output.shape}"

            # Verify output is finite
            assert torch.isfinite(
                output
            ).all(), f"Layer {input_size}->{output_size} produced non-finite values"

            # Verify layer statistics make sense
            stats = layer.get_layer_stats()
            assert stats["total_forward_calls"] == 1
            assert stats["state_stats"]["num_seeds"] == 2

        print(
            f"Successfully tested {len(layer_configs)} different layer configurations"
        )

    def test_memory_usage_scaling(self):
        """Test memory usage with different layer sizes."""
        layer_configs = [
            (128, 64, 2),
            (256, 128, 2),
            (512, 256, 4),
            (1024, 512, 4),
        ]

        memory_usage = []

        for input_size, output_size, num_seeds in layer_configs:
            # Clear GPU cache if available
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            layer = KasminaLayer(
                input_size=input_size,
                output_size=output_size,
                num_seeds=num_seeds,
                telemetry_enabled=False,
            )

            # Estimate memory usage (parameters + state)
            param_memory = sum(p.numel() * p.element_size() for p in layer.parameters())
            state_memory = (
                layer.state_layout.alpha_blend.numel()
                * layer.state_layout.alpha_blend.element_size()
            )
            total_memory = param_memory + state_memory

            memory_usage.append(total_memory)

            print(
                f"Layer {input_size}â†’{output_size} (seeds={num_seeds}): {total_memory / 1024 / 1024:.2f} MB"
            )

        # Memory should scale reasonably with layer size
        for i in range(1, len(memory_usage)):
            prev_config = layer_configs[i - 1]
            curr_config = layer_configs[i]

            # Calculate expected scaling based on parameter count
            prev_params = prev_config[0] * prev_config[1] + prev_config[1]  # W + b
            curr_params = curr_config[0] * curr_config[1] + curr_config[1]
            expected_ratio = curr_params / prev_params

            actual_ratio = memory_usage[i] / memory_usage[i - 1]

            # Allow some overhead for state management
            assert (
                actual_ratio < expected_ratio * 1.5
            ), f"Memory scaling {actual_ratio:.2f} exceeds expected {expected_ratio:.2f}"


@pytest.mark.performance
class TestModelPerformance:
    """Test performance of complete morphable models."""

    def test_full_model_overhead_linear(self, simple_linear_model, performance_config):
        """Test overhead of complete linear model."""
        # Wrap the model
        morphable_model = esper.wrap(simple_linear_model, telemetry_enabled=False)

        input_tensor = torch.randn(32, 128)

        # Warm up
        for _ in range(10):
            _ = simple_linear_model(input_tensor)
            _ = morphable_model(input_tensor)

        # Measure original model
        start_time = time.perf_counter()
        for _ in range(200):
            original_output = simple_linear_model(input_tensor)
        original_time = time.perf_counter() - start_time

        # Measure morphable model
        start_time = time.perf_counter()
        for _ in range(200):
            morphable_output = morphable_model(input_tensor)
        morphable_time = time.perf_counter() - start_time

        # Verify outputs match
        assert torch.allclose(original_output, morphable_output, atol=1e-5)

        # Verify overhead is reasonable for full model
        overhead = (morphable_time - original_time) / original_time * 100
        # Use more lenient threshold for full models due to compounding overhead
        max_overhead = 30.0  # 30% is reasonable for full model wrapping

        assert (
            overhead < max_overhead
        ), f"Full model overhead {overhead:.2f}% exceeds {max_overhead}% target"

        print(f"Full linear model overhead: {overhead:.2f}%")

    def test_full_model_overhead_conv(self, simple_conv_model, performance_config):
        """Test overhead of complete convolutional model."""
        # Wrap the model
        morphable_model = esper.wrap(simple_conv_model, telemetry_enabled=False)

        input_tensor = torch.randn(8, 3, 32, 32)

        # Warm up
        for _ in range(5):
            _ = simple_conv_model(input_tensor)
            _ = morphable_model(input_tensor)

        # Measure original model
        start_time = time.perf_counter()
        for _ in range(50):
            original_output = simple_conv_model(input_tensor)
        original_time = time.perf_counter() - start_time

        # Measure morphable model
        start_time = time.perf_counter()
        for _ in range(50):
            morphable_output = morphable_model(input_tensor)
        morphable_time = time.perf_counter() - start_time

        # Verify outputs match
        assert torch.allclose(original_output, morphable_output, atol=1e-5)

        # Verify overhead is reasonable for full conv model
        overhead = (morphable_time - original_time) / original_time * 100
        # Use more lenient threshold for conv models due to memory patterns
        max_overhead = 30.0  # 30% is reasonable for full conv model wrapping

        assert (
            overhead < max_overhead
        ), f"Full Conv model overhead {overhead:.2f}% exceeds {max_overhead}% target"

        print(f"Full convolutional model overhead: {overhead:.2f}%")

    def test_concurrent_execution(self, performance_config):
        """Test performance under concurrent execution."""
        import threading

        # Create multiple models
        models = [
            esper.wrap(
                nn.Sequential(nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, 32)),
                telemetry_enabled=False,
            )
            for _ in range(4)
        ]

        input_tensors = [torch.randn(16, 128) for _ in range(4)]
        results = [None] * 4
        execution_times = [0.0] * 4

        def run_model(idx):
            model = models[idx]
            input_tensor = input_tensors[idx]

            # Warm up
            for _ in range(5):
                _ = model(input_tensor)

            # Measure
            start_time = time.perf_counter()
            for _ in range(50):
                output = model(input_tensor)
            end_time = time.perf_counter()

            results[idx] = output
            execution_times[idx] = end_time - start_time

        # Run concurrent executions
        threads = [threading.Thread(target=run_model, args=(i,)) for i in range(4)]

        start_time = time.perf_counter()
        for thread in threads:
            thread.start()
        for thread in threads:
            thread.join()
        total_time = time.perf_counter() - start_time

        # Verify all completed successfully
        assert all(r is not None for r in results)

        # Sequential execution time estimate
        sequential_time = sum(execution_times)

        # Concurrent should be faster than sequential (allowing for overhead)
        efficiency = sequential_time / total_time

        print(f"Concurrent execution efficiency: {efficiency:.2f}x")
        print(f"Individual execution times: {[f'{t:.3f}s' for t in execution_times]}")
        print(f"Total concurrent time: {total_time:.3f}s")

        # Should achieve some level of parallelism
        assert efficiency > 1.5, f"Concurrent efficiency {efficiency:.2f}x too low"


@pytest.mark.performance
class TestLatencyBenchmarks:
    """Benchmark latency characteristics."""

    def test_single_forward_pass_latency(self, performance_config):
        """Test single forward pass latency."""
        layer = KasminaLayer(
            input_size=512, output_size=256, num_seeds=4, telemetry_enabled=False
        )

        input_tensor = torch.randn(1, 512)

        # Warm up
        for _ in range(10):
            _ = layer(input_tensor)

        # Measure single pass latencies
        latencies = []
        for _ in range(100):
            start_time = time.perf_counter()
            _ = layer(input_tensor)
            end_time = time.perf_counter()
            latencies.append((end_time - start_time) * 1000)  # Convert to ms

        # Statistical analysis
        avg_latency = sum(latencies) / len(latencies)
        max_latency = max(latencies)
        p95_latency = sorted(latencies)[95]
        p99_latency = sorted(latencies)[99]

        max_acceptable_latency = performance_config["max_latency_ms"]

        print("Latency statistics (ms):")
        print(f"  Average: {avg_latency:.3f}")
        print(f"  P95: {p95_latency:.3f}")
        print(f"  P99: {p99_latency:.3f}")
        print(f"  Maximum: {max_latency:.3f}")

        # Verify latency requirements
        assert (
            avg_latency < max_acceptable_latency
        ), f"Average latency {avg_latency:.3f}ms exceeds {max_acceptable_latency}ms"
        assert (
            p99_latency < max_acceptable_latency * 2
        ), f"P99 latency {p99_latency:.3f}ms exceeds {max_acceptable_latency * 2}ms"

    def test_kernel_loading_latency(self, performance_config):
        """Test kernel loading latency (simulated)."""
        layer = KasminaLayer(
            input_size=256, output_size=128, num_seeds=4, telemetry_enabled=False
        )

        # Mock kernel loading with realistic data
        mock_kernel = torch.randn(128, 256)

        loading_times = []
        for seed_idx in range(4):
            # Simulate kernel loading process
            start_time = time.perf_counter()

            # Simulate the loading operations that would happen
            _ = mock_kernel.clone()  # Data copy
            # Simple state update without transition (just measure basic operations)
            layer.state_layout.alpha_blend[seed_idx] = 0.5  # Alpha update

            end_time = time.perf_counter()
            loading_times.append((end_time - start_time) * 1000)  # Convert to ms

        avg_loading_time = sum(loading_times) / len(loading_times)
        max_loading_time = max(loading_times)

        print("Kernel loading latency (ms):")
        print(f"  Average: {avg_loading_time:.3f}")
        print(f"  Maximum: {max_loading_time:.3f}")

        # Loading should be fast for real-time adaptation
        assert (
            avg_loading_time < 1.0
        ), f"Average kernel loading {avg_loading_time:.3f}ms too slow"
        assert (
            max_loading_time < 5.0
        ), f"Maximum kernel loading {max_loading_time:.3f}ms too slow"
